import pandas as pd 
import numpy as np
import pickle
import sklearn.ensemble as ske 
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn import tree, linear_model
from sklearn.feature_selection import SelectFromModel,SelectKBest, chi2
#from sklearn.externals import joblib
import joblib
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestRegressor 
import seaborn as sns 
import matplotlib.pyplot as plt 
from scipy.stats import norm

data = pd.read_csv('brazilian-malware-dataset-master/brazilian-malware.csv', sep=',')  

data.reset_index()          
A = data.drop(['FirstSeenDate', 'Identify', 'SHA1', 'PE_TYPE','ImportedDlls', 'ImportedSymbols'], axis=1)
X= A.values                                                            
y = data['Label'].values     

corrmat = data.corr() 

f, ax = plt.subplots(figsize =(9, 8)) 
sns.heatmap(corrmat, ax = ax, cmap ="YlGnBu", linewidths = 0.1) 
plt.show()

#print(A)

print('Finding most important features based on %i total features\n' % X.shape[1])           

# Feature selection using Trees Classifier
#fsel = ske.ExtraTreesClassifier().fit(X, y)
#X_new = SelectKBest(chi2, k=10).fit_transform(X, y)

X_new1 = SelectKBest(chi2, k=10).fit(X, y)

X_new=X_new1.transform(X)

#fit = best_features.fit(X,y)
df_scores = pd.DataFrame(X_new1.scores_)
df_columns = pd.DataFrame(A.columns)
# concatenate dataframes
feature_scores = pd.concat([df_columns, df_scores],axis=1)
feature_scores.columns = ['Feature_Name','Score']  # name output columns
print(feature_scores.nlargest(10,'Score'))  # print 10 best features
# export selected features to .csv
df_univ_feat = feature_scores.nlargest(10,'Score')
df_univ_feat.to_csv('feature_selection_UNIVARIATE.csv', index=False)

#print("Original features ",A.head())
#print("Important features ",X_new[0:5])

'''
model = SelectFromModel(fsel, prefit=True)   
X_new = model.transform(X)                                                               
'''
nb_features = X_new.shape[1]                                                                

#print(nb_features)

X_train, X_test, y_train, y_test = train_test_split(X_new, y ,test_size=0.15)             
#features1 = pd.read_csv("feature_selection_UNIVARIATE.csv",usecols=["Feature_Name"])

#features=features1.values.tolist()

print('%i features are found to be important:' % nb_features) 

'''
#important features sorted
indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. feature %s (%f)" % (f + 1, A.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))

# mean adding to the empty 'features' array the 'important features'
for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:nb_features]):#[::-1] mean start with last towards first 
    features.append(A.columns[2+f])
'''
#Algorithm comparison
algorithms = {
    
         "DecisionTree": tree.DecisionTreeClassifier(max_depth=10),
        #The max_depth parameter denotes maximum depth of the tree.

        "RandomForest": ske.RandomForestClassifier(n_estimators=50),#In case, of random forest, these ensemble classifiers are the randomly created decision trees.
         #n_estimators ==The number of trees in the forest.
    
        "GradientBoosting": ske.GradientBoostingClassifier(n_estimators=50),
        "AdaBoost": ske.AdaBoostClassifier(n_estimators=100),
        "XG Boost": XGBClassifier(n_estimators=50),
         #Ada mean Adaptive
         #Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner. 
    
        "Gaussian Naive Bayes": GaussianNB()
        #Bayes theorem is based on conditional probability.
    }

results = {}
print("\n Testing the classification algorithms")
for algo in algorithms:
    clf = algorithms[algo]
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("%s : %f %%" % (algo, score*100))
    results[algo] = score

best = max(results, key=results.get)
print('\n The best algorithm is %s with a %f %% success' % (best, results[best]*100))

# Save the algorithm and the feature list for later predictions
print('Saving algorithm and feature list..')
joblib.dump(algorithms[best], 'classifier/classifier.pkl')
#open('classifier/features.pkl', 'wb').write(pickle.dumps(features)) 
print('Saved')

# Identify false and true positive rates
clf = algorithms[best]
res = clf.predict(X_test)
#print(res)
'''
#Algorithm comparison
algorithms1 = {
    
         "DecisionTree": tree.DecisionTreeRegressor(max_depth=10),
        #The max_depth parameter denotes maximum depth of the tree.

        "RandomForest": RandomForestRegressor(n_estimators = 100, random_state = 0) ,
         #n_estimators ==The number of trees in the forest.
    
        "Linear Regression": linear_model.LinearRegression()
    }

results = {}
print("\n Testing the regression algorithms")
for algo in algorithms1:
    clf = algorithms1[algo]
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("%s : %f %%" % (algo, score*100))
    results[algo] = score

best = max(results, key=results.get)
print('\n The best regression algorithm is %s with a %f %% accuracy' % (best, results[best]*100))

# Save the algorithm and the feature list for later predictions
print('Saving algorithm and feature list..')
joblib.dump(algorithms[best], 'classifier/regressor.pkl')
print('Saved')

clf = algorithms[best]
res1 = clf.predict(X_test)

#print(res1)
'''
#confusion matrix
cm = confusion_matrix(y_test, res)
print("True positive rate : %f %%" % ((cm[0][0] / float(sum(cm[0])))*100))
print('True negative rate : %f %%' % ( (cm[1][1] / float(sum(cm[1]))*100)))

print("False positive rate : %f %%" % ((cm[0][1] / float(sum(cm[0])))*100))
print('False negative rate : %f %%' % ( (cm[1][0] / float(sum(cm[1]))*100)))
